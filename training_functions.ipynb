{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train_test_data(data, seq_length, ftd, ltd, all_days):\n",
    "    '''\n",
    "    Input: data, seq_length (length of time-series to in input), ftd: first training date (index), ltd: last training date (index)\n",
    "    Output:\n",
    "    data_train, x_train with shape (batchsize, seq_length, channels), y_train with shape (batchsize, ), data_test, x_test, y_test\n",
    "    ret_d_train, ret_d_test: daily percentage returns \n",
    "    '''\n",
    "\n",
    "    if ftd < 100:\n",
    "        print('not enough standardization days')\n",
    "    \n",
    "    first_st_day = all_days[ftd-100]\n",
    "    last_st_day = all_days[ftd-1]\n",
    "    first_train_day = all_days[ftd]\n",
    "    last_train_day = all_days[ltd]\n",
    "    first_test_day = all_days[ltd-seq_length+2]\n",
    "    last_test_day = all_days[ltd+seq_length]\n",
    "    print(f'Standardization data are from {first_st_day} to {last_st_day}')\n",
    "    print(f'Training data are from {first_train_day} to {last_train_day}')\n",
    "    print(f'Testing data are from {first_test_day} to {last_test_day}')\n",
    "    data_st = data[(data['datadate'] >= first_st_day) & (data['datadate'] <= last_st_day)].reset_index(drop=True)\n",
    "    data_train = data[(data['datadate'] >= first_train_day) & (data['datadate'] <= last_train_day)].reset_index(drop=True)\n",
    "    data_test = data[(data['datadate'] >= first_test_day) & (data['datadate'] <= last_test_day)].reset_index(drop=True)\n",
    "\n",
    "    # Standardize here (1. use the previous 100 trading days to standardize)\n",
    "    # df_train_mean = data_st[factors + ['tic']].groupby('tic').mean().reset_index()\n",
    "    # df_train_std = data_st[factors + ['tic']].groupby('tic').std().reset_index()\n",
    "    # df_test_mean = data_train[factors + ['tic']].groupby('tic').mean().reset_index()\n",
    "    # df_test_std = data_train[factors + ['tic']].groupby('tic').std().reset_index()\n",
    "    # df_train = pd.merge(data_train, df_train_mean, how='left', on=['tic'], suffixes=('', '_MEAN'))\n",
    "    # df_test = pd.merge(data_test, df_test_mean, how='left', on=['tic'], suffixes=('', '_MEAN'))\n",
    "    # for factor in factors:\n",
    "    #     df_train[factor] = df_train[factor] - df_train[f'{factor}_MEAN']\n",
    "    #     df_test[factor] = df_test[factor] - df_test[f'{factor}_MEAN']\n",
    "    # df_train = df_train[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'direction', 'sic']]\n",
    "    # df_test = df_test[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'direction', 'sic']]\n",
    "    # df_train = pd.merge(df_train, df_train_std, how='left', on=['tic'], suffixes=('', '_STD'))\n",
    "    # df_test = pd.merge(df_test, df_test_std, how='left', on=['tic'], suffixes=('', '_STD'))\n",
    "    # for factor in factors:\n",
    "    #     df_train[factor] = df_train[factor] / df_train[f'{factor}_STD']\n",
    "    #     df_test[factor] = df_test[factor] / df_test[f'{factor}_STD']\n",
    "    \n",
    "    # Standardize here (2. for both train and test, use the 100 days previous to the first train day)\n",
    "    df_mean = data_st[factors + ['tic']].groupby('tic').mean().reset_index()\n",
    "    df_std = data_st[factors + ['tic']].groupby('tic').std().reset_index()\n",
    "    df_train = pd.merge(data_train, df_mean, how='left', on=['tic'], suffixes=('', '_MEAN'))\n",
    "    df_test = pd.merge(data_test, df_mean, how='left', on=['tic'], suffixes=('', '_MEAN'))\n",
    "    for factor in factors:\n",
    "        df_train[factor] = df_train[factor] - df_train[f'{factor}_MEAN']\n",
    "        df_test[factor] = df_test[factor] - df_test[f'{factor}_MEAN']\n",
    "    df_train = df_train[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'sic']]\n",
    "    df_test = df_test[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'sic']]\n",
    "    df_train = pd.merge(df_train, df_std, how='left', on=['tic'], suffixes=('', '_STD'))\n",
    "    df_test = pd.merge(df_test, df_std, how='left', on=['tic'], suffixes=('', '_STD'))\n",
    "    for factor in factors:\n",
    "        df_train[factor] = df_train[factor] / df_train[f'{factor}_STD']\n",
    "        df_test[factor] = df_test[factor] / df_test[f'{factor}_STD']   \n",
    "\n",
    "    # Winsorize here\n",
    "    # df_train[factors] = df_train[factors].clip(lower=-3, upper=3)\n",
    "    # df_test[factors] = df_test[factors].clip(lower=-3, upper=3)\n",
    "    data_train = df_train[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'sic']]\n",
    "    data_test = df_test[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'sic']]\n",
    "    # Fill NA\n",
    "    for factor in factors:\n",
    "        data_train.loc[:, factor] = data_train[factor].fillna(0)\n",
    "        data_test.loc[:, factor] = data_test[factor].fillna(0)\n",
    "\n",
    "    # Compute how many training data we will have\n",
    "    all_train_days = list(data_train.datadate.unique())\n",
    "    all_test_days = list(data_test.datadate.unique())\n",
    "    num_train_days = len(all_train_days)\n",
    "    num_test_days = len(all_test_days)\n",
    "    num_train_data = (num_train_days - seq_length + 1) * nt\n",
    "    num_test_data = (num_test_days - seq_length + 1) * nt\n",
    "        \n",
    "    # Create training data\n",
    "    x_train = np.zeros((num_train_data, len(factors), seq_length))\n",
    "    y_train = np.zeros((num_train_data, ))\n",
    "    y_train_dir = np.zeros((num_train_data, ))\n",
    "    ret_d_train = np.zeros((num_train_data, ))\n",
    "    sic_train = np.zeros((num_train_data, ))\n",
    "    for i in range(num_train_days - seq_length + 1):\n",
    "        train_days = all_train_days[i : seq_length + i]\n",
    "        data_temp = data_train[data_train['datadate'].isin(train_days)]\n",
    "        # Convert dataframe data to three dimensional training data (ticker, factor, time-series data)\n",
    "        # This is 'channels_first' type of data in training!; will change to channel last later!\n",
    "        pivot_data = data_temp[factors+['datadate', 'tic']].pivot_table(index='tic', columns='datadate')\n",
    "        x_train[i*nt:(i+1)*nt, :, :] = pivot_data.values.reshape(nt, len(factors), seq_length)\n",
    "        y_train[i*nt:(i+1)*nt] = data_train[data_train['datadate'] == train_days[-1]]['rank'].values.reshape(nt, )\n",
    "        # y_train_dir[i*nt:(i+1)*nt] = data_train[data_train['datadate'] == train_days[-1]]['direction'].values.reshape(nt, )\n",
    "        ret_d_train[i*nt:(i+1)*nt] = data_train[data_train['datadate'] == train_days[-1]]['ret_d'].values.reshape(nt, )\n",
    "        # Get categorical input sic\n",
    "        sic_train[i*nt:(i+1)*nt] = data_train[data_train['datadate'] == train_days[-1]]['sic'].values.reshape(nt, )\n",
    "        sic_train = np.nan_to_num(sic_train)\n",
    "\n",
    "    # Create testing data\n",
    "    x_test = np.zeros((num_test_data, len(factors), seq_length))\n",
    "    y_test = np.zeros((num_test_data, ))\n",
    "    y_test_dir = np.zeros((num_test_data, ))\n",
    "    ret_d_test = np.zeros((num_test_data, ))\n",
    "    sic_test = np.zeros((num_test_data, ))\n",
    "    for i in range(num_test_days - seq_length + 1):\n",
    "        test_days = all_test_days[i : seq_length + i]\n",
    "        data_temp = data_test[data_test['datadate'].isin(test_days)]\n",
    "        pivot_data = data_temp[factors+['datadate', 'tic']].pivot_table(index='tic', columns='datadate')\n",
    "        x_test[i*nt:(i+1)*nt, :, :] = pivot_data.values.reshape(nt, len(factors), seq_length)\n",
    "        y_test[i*nt:(i+1)*nt] = data_test[data_test['datadate'] == test_days[-1]]['rank'].values.reshape(nt, )\n",
    "        # y_test_dir[i*nt:(i+1)*nt] = data_test[data_test['datadate'] == test_days[-1]]['direction'].values.reshape(nt, )\n",
    "        ret_d_test[i*nt:(i+1)*nt] = data_test[data_test['datadate'] == test_days[-1]]['ret_d'].values.reshape(nt, )\n",
    "        # Get categorical input sic\n",
    "        sic_test[i*nt:(i+1)*nt] = data_test[data_test['datadate'] == test_days[-1]]['sic'].values.reshape(nt, )\n",
    "        sic_test = np.nan_to_num(sic_test)\n",
    "    \n",
    "    # Reshape train/test data so that it is channels_last\n",
    "    x_train = np.transpose(x_train, (0, 2, 1))\n",
    "    x_test = np.transpose(x_test, (0, 2, 1))\n",
    "    \n",
    "    # Let the label start with 0 to align with sparse cross-entropy\n",
    "    y_train[:] = y_train[:] + 2\n",
    "    # y_train_dir[:] = y_train_dir[:] + 1\n",
    "    y_test[:] = y_test[:] + 2\n",
    "    # y_test_dir[:] = y_test_dir[:] + 1\n",
    "    \n",
    "    print(f'Training data have shape {x_train.shape}, {y_train.shape}')\n",
    "    print(f'Testing data have shape {x_test.shape}, {y_test.shape}')\n",
    "    \n",
    "    return data_train, x_train, y_train, data_test, x_test, y_test, ret_d_train, ret_d_test, sic_train, sic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0fbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train_test_data_regression(data, seq_length, ftd, ltd, all_days, factors):\n",
    "    '''\n",
    "    Processing techniques should be the same as the one above; remember to change them at the same time\n",
    "    '''\n",
    "\n",
    "    if ftd < 100:\n",
    "        print('not enough standardization days')\n",
    "    \n",
    "    first_st_day = all_days[ftd-100]\n",
    "    last_st_day = all_days[ftd-1]\n",
    "    first_train_day = all_days[ftd]\n",
    "    last_train_day = all_days[ltd]\n",
    "    first_test_day = all_days[ltd-seq_length+2]\n",
    "    last_test_day = all_days[ltd+seq_length]\n",
    "    print(f'Standardization data are from {first_st_day} to {last_st_day}')\n",
    "    print(f'Training data are from {first_train_day} to {last_train_day}')\n",
    "    print(f'Testing data are from {first_test_day} to {last_test_day}')\n",
    "    data_st = data[(data['datadate'] >= first_st_day) & (data['datadate'] <= last_st_day)].reset_index(drop=True)\n",
    "    data_train = data[(data['datadate'] >= first_train_day) & (data['datadate'] <= last_train_day)].reset_index(drop=True)\n",
    "    data_test = data[(data['datadate'] >= first_test_day) & (data['datadate'] <= last_test_day)].reset_index(drop=True)\n",
    "\n",
    "    # Standardize here (2. for both train and test, use the 100 days previous to the first train day)\n",
    "    df_mean = data_st[factors + ['tic']].groupby('tic').mean().reset_index()\n",
    "    df_std = data_st[factors + ['tic']].groupby('tic').std().reset_index()\n",
    "    df_train = pd.merge(data_train, df_mean, how='left', on=['tic'], suffixes=('', '_MEAN'))\n",
    "    df_test = pd.merge(data_test, df_mean, how='left', on=['tic'], suffixes=('', '_MEAN'))\n",
    "    for factor in factors:\n",
    "        df_train[factor] = df_train[factor] - df_train[f'{factor}_MEAN']\n",
    "        df_test[factor] = df_test[factor] - df_test[f'{factor}_MEAN']\n",
    "    df_train = df_train[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'sic']]\n",
    "    df_test = df_test[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'sic']]\n",
    "    df_train = pd.merge(df_train, df_std, how='left', on=['tic'], suffixes=('', '_STD'))\n",
    "    df_test = pd.merge(df_test, df_std, how='left', on=['tic'], suffixes=('', '_STD'))\n",
    "    for factor in factors:\n",
    "        df_train[factor] = df_train[factor] / df_train[f'{factor}_STD']\n",
    "        df_test[factor] = df_test[factor] / df_test[f'{factor}_STD']   \n",
    "\n",
    "    # Winsorize here\n",
    "    # df_train[factors] = df_train[factors].clip(lower=-3, upper=3)\n",
    "    # df_test[factors] = df_test[factors].clip(lower=-3, upper=3)\n",
    "    data_train = df_train[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'sic']]\n",
    "    data_test = df_test[['datadate', 'tic', 'conm'] + factors + ['ret_d', 'TBill3m', 'excess_ret_d', 'rel_ret_d', 'rank', 'sic']]\n",
    "    # Fill NA\n",
    "    for factor in factors:\n",
    "        data_train.loc[:, factor] = data_train[factor].fillna(0)\n",
    "        data_test.loc[:, factor] = data_test[factor].fillna(0)\n",
    "\n",
    "    y_train = np.array(data_train['ret_d'])\n",
    "    y_test = np.array(data_test['ret_d'])\n",
    "\n",
    "    return data_train, y_train, data_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def perform_regression(name, j, x_test_temp, day, ):\n",
    "    \n",
    "    # y_pred = regressor_dict[f'{name}_{j}'].predict(x_test_temp)\n",
    "    # top_indices = np.argsort(y_pred)[-num_stocks:]\n",
    "    # top_stocks = [num_to_tic_dict[num] for num in top_indices]\n",
    "    # print(f'top_stocks by {name} factors {j} to buy on {buy_day} are {top_stocks}')\n",
    "    # # Simulate investment\n",
    "    # total_asset, total, position_dict = invest(day, test_day, buy_day, simul_day, total_dict[f'{name}_{j}'],\n",
    "    #                                             total_asset_dict[f'{name}_{j}'], position_dict_all[f'{name}_{j}'], top_stocks)\n",
    "    # total_dict[f'{name}_{j}'] = total\n",
    "    # total_asset_dict[f'{name}_{j}'] = total_asset\n",
    "    # position_dict_all[f'{name}_{j}'] = position_dict   \n",
    "    # # Calculate SR PT1\n",
    "    # pred = sum([y_pred[ind] for ind in top_indices]) / num_stocks\n",
    "    # pred_avg_dict[f'{name}_{j}'] += pred\n",
    "    # pred_avg_period_dict[f'{name}_{j}'] += pred\n",
    "    # excess_ret_d = data_test[(data_test['datadate'] == test_day) & (data_test['tic'].isin(top_stocks))]['excess_ret_d'].values.mean()\n",
    "    # excess_return_period_dict[f'{name}_{j}'][0, day] = excess_ret_d\n",
    "    # excess_return_dict[f'{name}_{i}'][0, day] = excess_ret_d\n",
    "    \n",
    "    # # Ridge\n",
    "    # y_pred = ridge_dict[i].predict(x_test_temp)\n",
    "    # top_indices = np.argsort(y_pred)[-num_stocks:]\n",
    "    # top_stocks = [num_to_tic_dict[num] for num in top_indices]\n",
    "    # print(f'top_stocks by OLS factors {OLS_factors} to buy on {buy_day} are {top_stocks}')\n",
    "    # # Simulate investment\n",
    "    # total_asset, total, position_dict = invest(day, test_day, buy_day, simul_day, total_dict[f'ridge_{i}'],\n",
    "    #                                             total_asset_dict[f'ridge_{i}'], position_dict_all[f'ridge_{i}'], top_stocks)\n",
    "    # total_dict[f'ridge_{i}'] = total\n",
    "    # total_asset_dict[f'ridge_{i}'] = total_asset\n",
    "    # position_dict_all[f'ridge_{i}'] = position_dict   \n",
    "    # # Calculate SR PT1\n",
    "    # pred = sum([y_pred[ind] for ind in top_indices]) / num_stocks\n",
    "    # pred_avg_dict[f'ridge_{i}'] += pred\n",
    "    # pred_avg_ridge += pred\n",
    "    # excess_ret_d = data_test[(data_test['datadate'] == test_day) & (data_test['tic'].isin(top_stocks))]['excess_ret_d'].values.mean()\n",
    "    # excess_return_ridge[0, day] = excess_ret_d\n",
    "    # excess_return_dict[f'ridge_{i}'][0, day] = excess_ret_d\n",
    "    \n",
    "    # # Lasso\n",
    "    # y_pred = lasso_dict[i].predict(x_test_temp)\n",
    "    # top_indices = np.argsort(y_pred)[-num_stocks:]\n",
    "    # top_stocks = [num_to_tic_dict[num] for num in top_indices]\n",
    "    # print(f'top_stocks by OLS factors {OLS_factors} to buy on {buy_day} are {top_stocks}')\n",
    "    # # Simulate investment\n",
    "    # total_asset, total, position_dict = invest(day, test_day, buy_day, simul_day, total_dict[f'lasso_{i}'],\n",
    "    #                                             total_asset_dict[f'lasso_{i}'], position_dict_all[f'lasso_{i}'], top_stocks)\n",
    "    # total_dict[f'lasso_{i}'] = total\n",
    "    # total_asset_dict[f'lasso_{i}'] = total_asset\n",
    "    # position_dict_all[f'lasso_{i}'] = position_dict   \n",
    "    # # Calculate SR PT1\n",
    "    # pred = sum([y_pred[ind] for ind in top_indices]) / num_stocks\n",
    "    # pred_avg_dict[f'lasso_{i}'] += pred\n",
    "    # pred_avg_lasso += pred\n",
    "    # excess_ret_d = data_test[(data_test['datadate'] == test_day) & (data_test['tic'].isin(top_stocks))]['excess_ret_d'].values.mean()\n",
    "    # excess_return_lasso[0, day] = excess_ret_d\n",
    "    # excess_return_dict[f'lasso_{i}'][0, day] = excess_ret_d\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
