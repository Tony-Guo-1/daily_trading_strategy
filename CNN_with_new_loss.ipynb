{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "%run utils.py\n",
    "\n",
    "import random\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3848c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is connected\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.config.list_physical_devices('CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For importing processed data\n",
    "# processed_data_path = file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e012324",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce80c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basic_factors = ['cshtrd', 'prccd', 'prchd', 'prcld', 'prcod', 'dol_vol', 'Mom_2day', 'Mom_3day', 'Mom_5day',\n",
    "                 'MA_10day', 'MA_50day', 'open/MA10', 'open/MA50', 'STD_10day', 'H-L', 'RSI', 'MACD', 'MACD_Signal_Line']\n",
    "\n",
    "TA_factors = [# Momentum indicators\n",
    "              'momentum_stoch_rsi', 'momentum_stoch', 'momentum_ao', 'momentum_pvo', 'momentum_kama', 'momentum_wr',\n",
    "              # Volume indicators\n",
    "              'volume_adi', 'volume_em', 'volume_fi', 'volume_cmf', 'volume_vpt',\n",
    "              # Volatility indicators\n",
    "              'volatility_atr', 'volatility_bbh', 'volatility_dcw', 'volatility_ui',\n",
    "              # Trend indicators\n",
    "              'trend_adx', 'trend_aroon_up', 'trend_aroon_down', 'trend_ichimoku_a',\n",
    "              # Other indicators\n",
    "              'others_dr'\n",
    "]\n",
    "\n",
    "# Remove factors that have low variance\n",
    "basic_factors.remove('dol_vol')\n",
    "\n",
    "# Remove factors that have high correlation with the prcod (and prcod since it shouldn't affect the return)\n",
    "basic_factors.remove('prccd')\n",
    "basic_factors.remove('prcld')\n",
    "basic_factors.remove('prchd')\n",
    "basic_factors.remove('prcod')\n",
    "basic_factors.remove('MA_10day')\n",
    "TA_factors.remove('trend_ichimoku_a')\n",
    "TA_factors.remove('volatility_bbh')\n",
    "TA_factors.remove('momentum_kama')\n",
    "\n",
    "# Remove factors that have high correlation with momentum_stoch\n",
    "TA_factors.remove('momentum_wr')\n",
    "\n",
    "factors = basic_factors + TA_factors\n",
    "print(f'There are {len(basic_factors)} basic factors')\n",
    "print(f'There are {len(TA_factors)} TA factors')\n",
    "print(f'There are {len(factors)} factors')\n",
    "\n",
    "data = pd.read_csv(processed_data_path)\n",
    "all_days = list(data['datadate'].unique())\n",
    "num_of_tokens = data.sector.nunique()\n",
    "num_to_tic_dict, tic_to_num_dict = num_tic_dicts(data)\n",
    "\n",
    "data = remove_dead_stocks(data)\n",
    "\n",
    "data = assign_class_labels(data, 'fixed_thres')\n",
    "data = data[['datadate', 'tic'] + factors + ['ret_d', 'TBill1y', 'rel_ret_d', 'rank', 'sector']]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data distribution\n",
    "# data['rank'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60488060",
   "metadata": {},
   "source": [
    "# First Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa320b63",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to save the model after every training\n",
    "save = True\n",
    "\n",
    "# Set Seed\n",
    "seed = 120\n",
    "\n",
    "all_days = list(data['datadate'].unique())\n",
    "num_of_ts = len(all_days)\n",
    "batch_size = 4096 # Only for model.predict()\n",
    "print(f'There are {num_of_ts} days in the dataset')\n",
    "\n",
    "tickers = list(data.tic.unique())\n",
    "nt = len(tickers)\n",
    "print(f'There are {nt} tickers')\n",
    "assert len(tickers) * num_of_ts == data.shape[0]\n",
    "\n",
    "seq_length = 20 # Length of time-series\n",
    "train_length = 200 # Length of training data\n",
    "ftd = train_length # First train day\n",
    "ltd = ftd+train_length-1 # Last train day\n",
    "num_stocks = 10 # Choose the top {num_stocks} each day\n",
    "num_of_models = 3\n",
    "\n",
    "num_iters = math.floor((num_of_ts - 2*train_length) / seq_length)\n",
    "print(f'There are {num_iters} iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2bd3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train, x_train, y_train, data_test, x_test, y_test, ret_d_train, ret_d_test, sector_train, sector_test = prep_train_test_data(data, seq_length, ftd, ltd, all_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    # For training and validation\n",
    "    'num_models': 3,\n",
    "    'train_patience': 20,\n",
    "    'plateau_patience': 5,\n",
    "    'retrain_patience': 20,\n",
    "    'validation_split': 0.2,\n",
    "    'learning_rate': 0.01,\n",
    "    'retrain_learning_rate': 0.01,\n",
    "    'min_learning_rate': 0.001,\n",
    "    'epochs': 500,\n",
    "    'batch_size': 1024,\n",
    "    'verbose': 0,\n",
    "    # Model inputs\n",
    "    'target': keras.layers.Input(shape=1),\n",
    "    'ret_d': keras.layers.Input(shape=1),\n",
    "    'sector_input': keras.layers.Input(shape=1),\n",
    "    # Categorical input dimension\n",
    "    'embedding_dim': len(factors),\n",
    "    # Convolution parameters\n",
    "    'num_of_tokens': num_of_tokens,\n",
    "    'filter_dims': [64, 128],\n",
    "    'kernel_sizes': [4, 5],\n",
    "    'strides': [2, 2],\n",
    "    'paddings': [\"valid\", \"valid\"],\n",
    "    # Dense layer parameters\n",
    "    'layer_dims': [32, 8],\n",
    "    'output_dim': 5,\n",
    "    'activation': \"leaky_relu\",\n",
    "    # Regularization parameter\n",
    "    'dropout_conv': 0.35,\n",
    "    'dropout_dense': 0.35\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51af17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNN_model = CNN(x_train.shape[1:], seed, **model_params)\n",
    "\n",
    "# Visualize the model architecture\n",
    "# keras.utils.plot_model(CNN_model.model_dict[0], \"pic.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4241069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "CNN_model.compile_model()\n",
    "CNN_model.train_model(x_train, y_train, ret_d_train, sector_train) \n",
    "CNN_model.evaluate_model(x_train, y_train, ret_d_train, sector_train, x_test, y_test, ret_d_test, sector_test, batch_size)\n",
    "\n",
    "if save:\n",
    "    for i in range(model_params['num_models']):\n",
    "        CNN_model.model_dict[i].save(f\"models/model_{i}_{all_days[ftd]}_{all_days[ltd]}.keras\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cbe0d5",
   "metadata": {},
   "source": [
    "## Simulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18352f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_run = True\n",
    "total_dict = {}\n",
    "total_asset_dict = {}\n",
    "position_dict_all = {}\n",
    "num_iter = 0\n",
    "for i in range(num_of_models):\n",
    "    total_dict[i] = 1\n",
    "    total_asset_dict[i] = [1]\n",
    "    position_dict_all[i] = {}\n",
    "total_dict['ensemble_weighted'] = 1\n",
    "total_asset_dict['ensemble_weighted'] = [1]\n",
    "position_dict_all['ensemble_weighted'] = {}\n",
    "total_dict['ensemble_equal'] = 1\n",
    "total_asset_dict['ensemble_equal'] = [1]\n",
    "position_dict_all['ensemble_equal'] = {}\n",
    "\n",
    "return_dict = {}\n",
    "total_asset_dict, total_dict, position_dict_all, return_dict = simulate(ftd, ltd, total_dict, first_run, num_stocks, total_asset_dict, position_dict_all, return_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d381f52",
   "metadata": {},
   "source": [
    "# Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837af8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrain all the rest\n",
    "print(f'Running {num_iters} iterations; first iteration already completed.')\n",
    "first_run = False\n",
    "\n",
    "for num_iter in range(1, num_iters):\n",
    "    # Find train and test data\n",
    "    ftd += seq_length\n",
    "    ltd += seq_length\n",
    "    data_train, x_train, y_train, data_test, x_test, y_test, ret_d_train, ret_d_test, sector_train, sector_test = prep_train_test_data(data, seq_length, ftd, ltd, all_days)\n",
    "    print(f'Running iteration {num_iter+1} out of {num_iters} iterations')\n",
    "    CNN_model.retrain_model(x_train, y_train, ret_d_train, sector_train) \n",
    "    CNN_model.evaluate_model(x_train, y_train, ret_d_train, sector_train, x_test, y_test, ret_d_test, sector_test, batch_size)\n",
    "\n",
    "    if save:\n",
    "        for i in range(model_params['num_models']):\n",
    "            CNN_model.model_dict[i].save(f\"models/model_{i}_{all_days[ftd]}_{all_days[ltd]}.keras\")\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    total_asset_dict, total_dict, position_dict_all, return_dict = simulate(ftd, ltd, total_dict, first_run, num_stocks, total_asset_dict, \n",
    "                                                                            position_dict_all, return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc88906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or load portfolio values\n",
    "# with open('total_asset_dict.json', 'w') as f:\n",
    "#     json.dump(total_asset_dict, f)\n",
    "    \n",
    "# with open('total_asset_dict.json', 'r') as fr:\n",
    "#     test = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b557d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_simul_day = \"2024-12-03\"\n",
    "rfr = list(data[data['datadate'] <= last_simul_day][['datadate', 'TBill1y']].drop_duplicates(subset=['datadate'])['TBill1y'])\n",
    "rfr = rfr[-(len(total_asset_dict[0])-1):]\n",
    "\n",
    "def calculate_SR(total_asset, rfr):\n",
    "    '''\n",
    "    Calculates the Sharpe Ratio of the simulated strategy\n",
    "    Inputs:\n",
    "        total_asset: list of total assets on each day\n",
    "        rfr: list of risk-free rates on the same days as in total_asset\n",
    "    Output:\n",
    "        SR: Sharpe Ratio\n",
    "    '''\n",
    "    \n",
    "    daily_ret = [(total_asset[i] - total_asset[i-1]) / total_asset[i-1] for i in range(1, len(total_asset))]\n",
    "    assert len(daily_ret) == len(rfr)\n",
    "    excess_ret = [daily_ret[i] - rfr[i] for i in range(len(rfr))]\n",
    "    SR = np.mean(excess_ret) / np.std(daily_ret) * np.sqrt(252)\n",
    "    \n",
    "    return SR\n",
    "for key, item in total_asset_dict.items():\n",
    "    SR = calculate_SR(item, rfr)\n",
    "    print(f'SR for model {key} is {SR}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
