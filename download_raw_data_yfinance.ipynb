{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864b3b23",
   "metadata": {},
   "source": [
    "This is a demonstration on how to obtain the data for the 2018-2024 period deposited at Mendeley data (https://data.mendeley.com/datasets/czwwfgcgz7/1). The data for the other period is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50251131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ta import add_all_ta_features\n",
    "import time\n",
    "\n",
    "%run utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9e14e",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34ca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2270 tickers\n"
     ]
    }
   ],
   "source": [
    "with open(\"2019-2024-tics-WRDS.txt\", \"r\") as f:\n",
    "    tickers = [tic.rstrip() for tic in f]\n",
    "\n",
    "# Some of the tickers will fail\n",
    "# failed_tics = ['BAC.PL', 'BF.B', 'BIG', 'BRK.A', 'BRK.B', 'DCPH', 'EBIXQ', 'ETRN', 'EURN', 'EVBG', 'GPS', 'GTHX', 'HA', 'HEI.A',\n",
    "#                'HIBB', 'LGF.A', 'LSXMA', 'LSXMK', 'MOG.A', 'NSTGQ', 'PBR.A', 'PNM', 'PRFT', 'SIX', 'SLCA', 'SPWR', 'SWN', 'TUP',\n",
    "#                'TWOU', 'UCBI', 'VGR', 'WIRE', 'WRK']\n",
    "for tic in failed_tics:\n",
    "    tickers.remove(tic)\n",
    "tickers = tickers + ['BAC-PL', 'BF-B', 'BRK-A', 'BRK-B', 'HEI-A', 'LGF-A', 'MOG-A', 'PBR-A']\n",
    "print(f\"There are {len(tickers)} tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6c5f5",
   "metadata": {},
   "source": [
    "yfinance only accepts up to 2000 requests per hour per IP (reference: https://stackoverflow.com/questions/5888662/does-yahoo-finance-have-data-request-upper-limit-is-there-an-alternative-or-wor/32913242#32913242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2d24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "missing_sector = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    tic_data = yf.download(ticker, start=\"2018-01-03\", end=\"2024-12-06\", group_by='ticker')\n",
    "    tic_data = tic_data.stack(level=0, future_stack=True).reset_index()\n",
    "    # Sleep for 3 seconds so there should be no more than 1200 requests per hour\n",
    "    time.sleep(3)\n",
    "    \n",
    "    stock = yf.Ticker(ticker).info\n",
    "    try:\n",
    "        # Sector information\n",
    "        tic_data['sector'] = stock['sector']\n",
    "        missing_sector.append(ticker)\n",
    "    except:\n",
    "        tic_data['sector'] = 'other'\n",
    "        print(f'{ticker} has missing sector')\n",
    "    data = pd.concat([data, tic_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of failed requests in the previous cell\n",
    "failed_tics = set(tickers) - set(data.Ticker.unique())\n",
    "\n",
    "while len(failed_tics) > 0:\n",
    "    other_data = pd.DataFrame()\n",
    "    for ticker in failed_tics:\n",
    "        tic_data = yf.download(ticker, start=\"2018-01-03\", end=\"2024-12-06\", group_by='ticker')\n",
    "        tic_data = tic_data.stack(level=0, future_stack=True).reset_index()\n",
    "        time.sleep(3)\n",
    "    \n",
    "        stock = yf.Ticker(ticker).info\n",
    "        try:\n",
    "            # Sector information\n",
    "            tic_data['sector'] = stock['sector']\n",
    "            missing_sector.append(ticker)\n",
    "        except:\n",
    "            tic_data['sector'] = 'other'\n",
    "            print(f'{ticker} has missing info')\n",
    "        other_data = pd.concat([other_data, tic_data])\n",
    "    data = pd.concat([data, other_data])\n",
    "    failed_tics = set(tickers) - set(data.Ticker.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = {'Date': 'datadate', 'Ticker': 'tic', 'Open': 'prcod', 'High': 'prchd', 'Low': 'prcld', 'Close': 'prccd',\n",
    "                'Volume': 'cshtrd'}\n",
    "data = data.rename(columns=column_names)\n",
    "data = data[['datadate', 'tic', 'prcod', 'prchd', 'prcld', 'prccd', 'cshtrd', 'sector']]\n",
    "data = data.reset_index(drop=True)\n",
    "data.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f981aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dates = set(data['datadate'].unique())\n",
    "apple_dates = set(data[data['tic']=='AAPL']['datadate'].unique())\n",
    "print(len(apple_dates))\n",
    "print(len(total_dates - apple_dates))\n",
    "\n",
    "num_of_ts = data[data['tic']=='AAPL']['datadate'].nunique()\n",
    "print(f'There are {num_of_ts} timestamps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_tic(data, num_of_ts)\n",
    "\n",
    "# Check that each ticker has exactly num_of_ts dates\n",
    "tickers = list(data.tic.unique())\n",
    "assert data.shape[0] == num_of_ts * len(tickers)\n",
    "\n",
    "data = remove_low_dollar_vol(data, dol_vol_thres = 10000000)\n",
    "data\n",
    "\n",
    "# Save tickers\n",
    "# with open(\"2018-2024-tics-yh.txt\", \"w\") as f:\n",
    "#     for tic in tickers:\n",
    "#         f.write(f\"{tic}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656a37e",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TBill path (for calculating risk-free rates; optional)\n",
    "TBill_path = '1yearTBill_all_times.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['datadate'] = pd.to_datetime(data['datadate'])\n",
    "data = compute_ret(data, TBill_path)\n",
    "data = remove_dead_stocks(data)\n",
    "\n",
    "data = assign_class_labels(data, 'fixed_size')\n",
    "# Make sector column\n",
    "data, num_of_tokens, num_to_sector_dict = make_sector_column(data)\n",
    "# Create dictionary that associate each ticker with a numerical label and vice versa for easier reference\n",
    "num_to_tic_dict, tic_to_num_dict = num_tic_dicts(data)\n",
    "\n",
    "print(f'Confirm that data has no NAs: {~data.isna().any().any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda83104",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = ['cshtrd', 'prccd', 'prchd', 'prcld', 'prcod', 'dol_vol']\n",
    "\n",
    "data, factors = feature_engineer(data, factors)\n",
    "print(f'Confirm that feature engineering did not create NaNs: {~data.isna().any().any()}')\n",
    "print(f'All factors: {factors}')\n",
    "\n",
    "assert data.shape[0] == data.datadate.nunique() * data.tic.nunique()\n",
    "\n",
    "data = all_features_ta(data)\n",
    "print(f'Confirm that data has no NaNs: {~data.isna().any().any()}')\n",
    "\n",
    "TA_factors = [# Momentum indicators\n",
    "              'momentum_stoch_rsi', 'momentum_stoch', 'momentum_ao', 'momentum_pvo', 'momentum_kama', 'momentum_wr',\n",
    "              # Volume indicators\n",
    "              'volume_adi', 'volume_em', 'volume_fi', 'volume_cmf', 'volume_vpt',\n",
    "              # Volatility indicators\n",
    "              'volatility_atr', 'volatility_bbh', 'volatility_dcw', 'volatility_ui',\n",
    "              # Trend indicators\n",
    "              'trend_adx', 'trend_aroon_up', 'trend_aroon_down', 'trend_ichimoku_a',\n",
    "              # Other indicators\n",
    "              'others_dr'\n",
    "]\n",
    "\n",
    "factors.extend(TA_factors)\n",
    "print(factors)\n",
    "\n",
    "data = data[['datadate', 'tic'] + factors + ['ret_d', 'TBill1y', 'rel_ret_d', 'DistinctRank', 'rank', 'sector']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc48c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "# data.to_csv('YH_processed_20180103-20241206.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
